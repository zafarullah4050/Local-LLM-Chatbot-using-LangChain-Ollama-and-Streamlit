# Local-LLM-Chatbot-using-LangChain-Ollama-and-Streamlit
I built an interactive chatbot powered by a local LLM (Large Language Model) using the following stack:

🔗 LangChain for prompt management and chaining

🧠 Ollama to run the local gemma:2b model (no API calls needed)

🌐 Streamlit for a real-time web-based chat interface

🔐 .env support using python-dotenv for secure key and config handling

📌 Features:

Simple and clean UI using Streamlit

Real-time Q&A powered by a local model (gemma:2b)

No dependency on external APIs like OpenAI – fully private & local

Environment setup using .env for flexibility

Easily extendable to other models like mistral, llama2, etc.
