# Local-LLM-Chatbot-using-LangChain-Ollama-and-Streamlit
I built an interactive chatbot powered by a local LLM (Large Language Model) using the following stack:

ğŸ”— LangChain for prompt management and chaining

ğŸ§  Ollama to run the local gemma:2b model (no API calls needed)

ğŸŒ Streamlit for a real-time web-based chat interface

ğŸ” .env support using python-dotenv for secure key and config handling

ğŸ“Œ Features:

Simple and clean UI using Streamlit

Real-time Q&A powered by a local model (gemma:2b)

No dependency on external APIs like OpenAI â€“ fully private & local

Environment setup using .env for flexibility

Easily extendable to other models like mistral, llama2, etc.
